{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Tweets\n",
    "\n",
    "A tweet consists of many data fields. [Here is an example](https://gist.github.com/arapat/03d02c9b327e6ff3f6c3c5c602eeaf8b). You can learn all about them in the Twitter API doc. We are going to briefly introduce only the data fields that will be used in this homework.\n",
    "\n",
    "* `created_at`: Posted time of this tweet (time zone is included)\n",
    "* `id_str`: Tweet ID - we recommend using `id_str` over using `id` as Tweet IDs, becauase `id` is an integer and may bring some overflow problems.\n",
    "* `text`: Tweet content\n",
    "* `user`: A JSON object for information about the author of the tweet\n",
    "    * `id_str`: User ID\n",
    "    * `name`: User name (may contain spaces)\n",
    "    * `screen_name`: User screen name (no spaces)\n",
    "* `retweeted_status`: A JSON object for information about the retweeted tweet (i.e. this tweet is not original but retweeteed some other tweet)\n",
    "    * All data fields of a tweet except `retweeted_status`\n",
    "* `entities`: A JSON object for all entities in this tweet\n",
    "    * `hashtags`: An array for all the hashtags that are mentioned in this tweet\n",
    "    * `urls`: An array for all the URLs that are mentioned in this tweet\n",
    "\n",
    "\n",
    "## Data source\n",
    "\n",
    "All tweets are collected using the [Twitter Streaming API](https://dev.twitter.com/streaming/overview).\n",
    "\n",
    "\n",
    "## Users partition\n",
    "\n",
    "Besides the original tweets, we will provide you with a Pickle file, which contains a partition over 452,743 Twitter users. It contains a Python dictionary `{user_id: partition_id}`. The users are partitioned into 7 groups."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 0: Load data to a RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tweets data is stored on AWS S3. We have in total a little over 1 TB of tweets. We provide 10 MB of tweets for your local development. For the testing and grading on the homework server, we will use different data.\n",
    "\n",
    "## Testing on the homework server\n",
    "In the Playground, we provide three different input sizes to test your program: 1 GB, 10 GB, and 100 GB. To test them, read files list from `../Data/hw2-files-1gb.txt`, `../Data/hw2-files-5gb.txt`, `../Data/hw2-files-20gb.txt`, respectively.\n",
    "\n",
    "For final submission, make sure to read files list from `../Data/hw2-files-final.txt`. Otherwise your program will receive no points.\n",
    "\n",
    "## Local test\n",
    "\n",
    "For local testing, read files list from `../Data/hw2-files.txt`.\n",
    "Now let's see how many lines there are in the input files.\n",
    "\n",
    "1. Make RDD from the list of files in `hw2-files.txt`.\n",
    "2. Mark the RDD to be cached (so in next operation data will be loaded in memory) \n",
    "3. call the `print_count` method to print number of lines in all these files\n",
    "\n",
    "It should print\n",
    "```\n",
    "Number of elements: 2193\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Your code here\n",
    "with open('../Data/hw2-files.txt') as f:\n",
    "    files = [l.strip() for l in f.readlines()]\n",
    "rdd1 = sc.textFile(','.join(files)).cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Parse JSON strings to JSON objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python has built-in support for JSON.\n",
    "\n",
    "**UPDATE:** Python built-in json library is too slow. In our experiment, 70% of the total running time is spent on parsing tweets. Therefore we recommend using [ujson](https://pypi.python.org/pypi/ujson) instead of json. It is at least 15x faster than the built-in json library according to our tests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Broken tweets and irrelevant messages\n",
    "\n",
    "The data of this assignment may contain broken tweets (invalid JSON strings). So make sure that your code is robust for such cases.\n",
    "\n",
    "In addition, some lines in the input file might not be tweets, but messages that the Twitter server sent to the developer (such as [limit notices](https://dev.twitter.com/streaming/overview/messages-types#limit_notices)). Your program should also ignore these messages.\n",
    "\n",
    "*Hint:* [Catch the ValueError](http://stackoverflow.com/questions/11294535/verify-if-a-string-is-json-in-python)\n",
    "\n",
    "\n",
    "(1) Parse raw JSON tweets to obtain valid JSON objects. From all valid tweets, construct a pair RDD of `(user_id, text)`, where `user_id` is the `id_str` data field of the `user` dictionary (read [Tweets](#Tweets) section above), `text` is the `text` data field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import ujson\n",
    "\n",
    "def safe_parse(x):\n",
    "    try:\n",
    "        json_object = ujson.loads(x)\n",
    "    except ValueError, e:\n",
    "        pass # invalid json\n",
    "    else:\n",
    "        return json_object\n",
    "\n",
    "# your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rdd2 = rdd1.filter(lambda x: 'user' in x and 'id_str' in x and 'text' in x).map(safe_parse)\\\n",
    "        .map(lambda x:  x['text'].encode('utf-8')).cache()\n",
    "N = rdd2.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove Punctuation Stopwords And Strip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[u'jenn', u'abram', u'attract', u'donald', u'trump'],\n",
       " [u'truli',\n",
       "  u'\\u2018finest\\u2019',\n",
       "  u'tast',\n",
       "  u'test',\n",
       "  u'trump',\n",
       "  u'co',\n",
       "  u'nufzz8wqc3',\n",
       "  u'erictrump']]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## removing punctuation stopwords and https\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "stop_words =set(stopwords.words(\"english\"))\n",
    "stop_words|=set([\"edu\", \"com\", \"also\", \"still\", \"anyone\", \"cc\" , \"ca\", \"us\", \"much\", \"even\", \"would\", \"see\", \"rt\", 'is', 'of'])\n",
    "st = PorterStemmer()\n",
    "\n",
    "#IMP: Put yout nltk_data folder in /usr/local/share for it to work :)\n",
    "\n",
    "def getwords(tweet):\n",
    "    return [x for x in tweet if not ( x.startswith('https') or x in stop_words or x is ' ')]\n",
    "punc = string.punctuation+\"\\n\"\n",
    "table_t = string.maketrans(punc, \" \"*len(punc))  ##TODO empty word\n",
    "tweetNeat = rdd2.map(lambda s: s.translate(table_t).lower())\\\n",
    "            .map(lambda s: getwords(s.decode('utf-8').split(\" \"))).filter(lambda x: len(x)>0).map(lambda x: filter(None,x))\n",
    "tweetNeat.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Dictionary of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "370753"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## make dictionary \n",
    "vocab = tweetNeat.flatMap(lambda x: x).distinct().collect()\n",
    "V = len(vocab)\n",
    "V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6634"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabSmall = tweetNeat.flatMap(lambda x: x).map(lambda x : (x,1)).reduceByKey(lambda a, b: a+b)\\\n",
    "                .filter(lambda (c,v): v<0.5*N and v>100).collect()\n",
    "        #.map(lambda (c,v):(v,c)).sortByKey(False).filter(lambda (c,v): v>0.5*N or v<100).collect()\n",
    "    #.filter().map(lambda (c,v):v).take(10000) ## or filter c>50\n",
    "Vsmall = len(vocabSmall)\n",
    "Vsmall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Histogram of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Annotate words\n",
    "import numpy as np\n",
    "\n",
    "tweetHist = tweetNeat.map(lambda s: np.array([vocabSmall.index(k) for k in s if k in vocabSmall]))\\\n",
    "          .map(lambda y: np.bincount(y.astype(int), minlength=len(vocabSmall)))\n",
    "#tweetHist.take(2)\n",
    "N = tweetHist.count()\n",
    "N"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 : Gibbs Sampler for Naive Bayes\n",
    "Naive Bayes:\\\\ \n",
    "//Prior\n",
    "• For each latent class c ∈ {1,...,K} \n",
    "    • θ(c) ∼ Dirichlet(α)\n",
    "• π ∼ Dirichlet(β) //Likelihood\n",
    "• For each document d ∈ {1,...,N}\n",
    "    • Document latent class z(d) ∼ discrete(π) \n",
    "    • For each word i in document d\n",
    "        • w(d) ∼ discrete(θ(z(d))) //Sample a word i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sampleFromDiscrete(probs):\n",
    "    temp = random.random()\n",
    "    total =0\n",
    "    for i in range(len(probs)):\n",
    "        total+=probs[i]\n",
    "        if(temp<total):\n",
    "            return i\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Z initialised based on similarity measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "K=15 # NO of classes.\n",
    "alpha = np.ones(V)*0.1 #hyperparameter\n",
    "A = [alpha]*K\n",
    "beta = np.ones(K)\n",
    "pi = np.random.dirichlet(beta)\n",
    "pi1=pi\n",
    "theta = np.random.dirichlet(alpha, K)\n",
    "no_tweets = N\n",
    "doc_vec = tweetHist.collect()\n",
    "docs = set(range(0, N))\n",
    "z = [0]*N\n",
    "for i in range(K):\n",
    "    d = docs.pop()\n",
    "    z[d] = i\n",
    "    dist = norm(doc_vec - doc_vec[d], axis=1)\n",
    "    sorted_dist = argsort(dist)[0:N/K]\n",
    "    for k in sorted_dist:\n",
    "        docs.discard(k)\n",
    "        doc_vec[k] = [-inf]*len(doc_vec[0])\n",
    "        z[k] = i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gibbs Sampler (Naive Bayes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([155, 141, 171, 135, 118, 125, 139, 148, 129, 138, 137, 183, 149,\n",
       "       153, 122])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bincount(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[155 142 142 142 142 142 142 142 142 142 142 142 142 142 142]\n",
      "[129 140 142 145 144 150 138 144 147 141 145 143 143 143 149]\n",
      "[116 133 138 145 143 156 142 151 149 143 143 147 145 145 147]\n",
      "[ 98 132 138 139 143 159 141 157 153 147 148 147 147 147 147]\n",
      "[ 98 130 136 137 145 157 140 157 150 146 153 147 148 150 149]\n",
      "[ 93 126 135 141 141 160 143 159 149 148 155 150 145 148 150]\n",
      "[ 89 126 131 139 138 159 142 162 151 150 157 151 145 151 152]\n",
      "[ 84 124 134 138 140 160 147 161 150 150 154 149 147 154 151]\n",
      "[ 77 119 134 136 139 160 145 165 150 151 159 154 149 154 151]\n",
      "[ 70 120 131 138 140 158 149 163 152 152 158 156 148 154 154]\n",
      "[ 71 115 130 142 139 161 145 164 151 151 159 156 149 154 156]\n",
      "[ 63 113 129 144 141 159 145 166 149 152 160 156 150 155 161]\n",
      "[ 62 112 129 143 141 157 144 166 150 154 160 157 151 155 162]\n",
      "[ 63 111 130 140 139 159 145 164 152 155 160 158 154 155 158]\n",
      "[ 62 105 129 142 142 157 142 169 151 155 160 157 156 157 159]\n",
      "[ 60 104 129 141 140 156 145 171 152 158 164 154 154 160 155]\n",
      "[ 60 103 130 139 141 154 146 172 152 163 164 154 152 158 155]\n",
      "[ 56 102 127 138 142 154 144 176 153 171 162 152 150 161 155]\n",
      "[ 54 100 122 135 145 152 147 177 152 175 157 156 151 165 155]\n",
      "[ 50 100 122 134 149 155 146 174 154 172 158 155 152 168 154]\n",
      "[ 48 101 121 131 149 155 150 176 154 171 159 155 153 168 152]\n",
      "[ 43 102 119 131 152 153 150 178 153 170 159 155 153 169 156]\n",
      "[ 41  99 119 133 151 157 150 176 153 175 158 157 153 166 155]\n",
      "[ 40  98 116 132 156 156 150 175 151 175 161 155 156 167 155]\n",
      "[ 42  99 115 134 155 151 152 173 152 177 165 155 156 162 155]\n",
      "[ 41  97 110 137 155 153 152 174 150 180 161 157 154 164 158]\n",
      "[ 40  97 109 132 152 152 152 176 152 181 161 162 159 162 156]\n",
      "[ 38  97 111 134 150 151 150 174 153 182 163 161 158 165 156]\n",
      "[ 37  98 110 131 148 152 148 175 154 183 164 163 161 164 155]\n",
      "[ 38  98 111 132 149 153 146 179 152 184 162 163 161 161 154]\n",
      "[ 36  98 107 132 145 153 145 182 154 185 162 166 161 162 155]\n",
      "[ 35  97 108 131 145 156 145 183 154 185 163 162 164 161 154]\n",
      "[ 36  96 107 133 145 157 146 185 153 184 163 161 163 159 155]\n",
      "[ 35  96 105 132 143 157 147 187 154 186 164 159 164 160 154]\n",
      "[ 34  96 105 133 144 154 147 188 154 186 164 163 160 161 154]\n",
      "[ 33  98 105 133 144 153 147 187 154 188 164 160 158 163 156]\n",
      "[ 31  93 104 135 146 159 148 184 157 184 166 161 158 163 154]\n",
      "[ 30  93 105 135 151 158 149 186 155 185 161 158 160 163 154]\n",
      "[ 29  90 106 133 152 156 151 189 154 190 159 160 160 162 152]\n",
      "[ 28  87 107 134 150 155 148 189 153 189 162 161 164 163 153]\n",
      "[ 28  86 108 130 153 155 146 189 155 187 163 165 161 164 153]\n",
      "[ 27  87 107 129 157 156 145 186 156 186 165 166 161 162 153]\n",
      "[ 27  86 107 133 154 156 145 183 158 187 163 166 161 162 155]\n",
      "[ 26  83 105 135 150 155 146 188 159 188 166 166 160 162 154]\n",
      "[ 26  83 105 131 153 152 150 187 158 189 167 167 157 162 156]\n",
      "[ 26  80 105 133 154 154 147 189 156 190 165 168 160 161 155]\n",
      "[ 24  81 103 133 156 153 146 187 158 191 168 168 160 159 156]\n",
      "[ 24  80 105 136 154 152 146 190 158 187 165 166 165 160 155]\n",
      "[ 25  81 104 135 154 150 144 192 159 187 167 167 161 162 155]\n",
      "[ 25  81 107 138 156 146 145 194 162 186 165 164 159 160 155]\n",
      "[ 24  79 104 138 154 149 147 195 159 183 165 167 160 162 157]\n",
      "[ 24  77 103 139 156 146 149 197 157 187 168 167 159 159 155]\n",
      "[ 23  79 102 140 157 146 145 198 160 184 168 169 160 158 154]\n",
      "[ 23  83 104 137 154 146 149 201 158 181 168 170 160 155 154]\n",
      "[ 23  81 103 139 154 143 148 203 161 179 167 170 162 155 155]\n",
      "[ 23  80 102 139 152 142 146 208 161 182 165 171 163 156 153]\n",
      "[ 23  80 104 139 150 142 149 205 160 179 165 170 165 159 153]\n",
      "[ 23  80 102 140 152 140 146 206 159 182 166 172 163 158 154]\n",
      "[ 22  81 102 138 153 139 146 204 163 184 166 170 162 159 154]\n",
      "[ 22  81 102 137 152 139 143 205 164 183 166 172 164 159 154]\n",
      "[ 22  82 102 137 152 139 143 206 163 186 165 173 162 159 152]\n",
      "[ 22  78 101 141 153 137 144 205 164 185 164 172 162 161 154]\n",
      "[ 24  78 100 138 152 139 143 200 164 183 167 174 164 162 155]\n",
      "[ 24  76 103 142 151 139 144 202 164 179 167 175 162 159 156]\n",
      "[ 22  78 100 142 150 142 145 201 164 179 168 171 165 160 156]\n",
      "[ 22  78 100 141 153 145 140 202 163 181 167 169 168 157 157]\n",
      "[ 24  77  99 138 153 144 140 204 160 182 165 170 172 158 157]\n",
      "[ 23  76  98 136 158 144 142 200 161 185 166 171 169 159 155]\n",
      "[ 21  74  97 135 157 144 139 203 162 189 165 173 169 159 156]\n",
      "[ 22  73  98 130 154 147 137 202 164 190 168 175 166 160 157]\n",
      "[ 20  74  95 131 153 148 137 205 163 192 163 174 168 161 159]\n",
      "[ 20  72  98 133 152 146 140 204 164 187 161 177 170 160 159]\n",
      "[ 19  72  97 131 154 148 145 203 165 187 161 176 169 159 157]\n",
      "[ 20  71  98 130 152 147 143 204 166 191 161 175 168 159 158]\n",
      "[ 20  72  98 129 155 146 143 205 167 188 162 174 169 158 157]\n",
      "[ 20  71  98 131 155 148 138 204 168 186 164 174 171 158 157]\n",
      "[ 19  72  96 131 155 149 138 204 165 188 163 175 171 157 160]\n",
      "[ 19  73  93 131 152 147 141 204 165 191 164 173 170 159 161]\n",
      "[ 19  73  92 126 152 147 136 210 168 189 164 174 172 159 162]\n",
      "[ 20  71  93 128 152 147 137 205 166 191 165 171 174 161 162]\n",
      "[ 20  72  92 128 154 145 134 209 163 194 164 170 176 159 163]\n",
      "[ 20  72  93 128 157 145 134 205 164 196 162 172 173 156 166]\n",
      "[ 20  73  92 128 161 142 136 205 167 193 163 172 171 154 166]\n",
      "[ 20  70  91 127 163 142 137 205 169 194 164 174 170 154 163]\n",
      "[ 20  70  93 127 161 142 137 203 169 195 164 174 169 156 163]\n",
      "[ 20  72  94 130 158 141 141 200 169 195 162 174 170 155 162]\n",
      "[ 18  70  93 131 163 143 139 204 169 192 163 173 167 162 156]\n",
      "[ 18  66  95 132 164 143 138 206 168 189 165 172 169 161 157]\n",
      "[ 18  68  93 132 159 142 141 207 169 189 164 171 173 160 157]\n",
      "[ 18  66  92 132 157 140 140 207 174 192 163 173 170 159 160]\n",
      "[ 18  68  90 129 157 140 139 208 175 192 163 179 167 159 159]\n",
      "[ 19  69  89 124 158 142 140 204 176 190 163 180 171 160 158]\n",
      "[ 18  67  89 128 161 138 141 206 177 191 165 174 169 159 160]\n",
      "[ 17  67  90 125 158 139 141 210 177 193 165 176 167 159 159]\n",
      "[ 17  71  90 125 157 138 138 212 176 194 165 176 169 157 158]\n",
      "[ 17  69  89 127 156 139 138 213 175 198 163 172 171 157 159]\n",
      "[ 17  71  90 130 157 139 137 207 175 198 165 171 168 157 161]\n",
      "[ 17  71  89 127 156 141 139 209 175 196 168 169 167 157 162]\n",
      "[ 17  73  92 129 157 136 138 208 174 195 170 169 166 156 163]\n",
      "[ 17  71  93 131 158 138 138 207 175 192 170 170 164 155 164]\n",
      "[ 17  70  92 133 159 137 138 211 175 189 169 172 163 156 162]\n",
      "[ 17  70  90 134 156 138 139 210 176 191 166 171 164 160 161]\n",
      "[ 17  70  89 129 159 141 138 211 173 190 168 172 163 163 160]\n",
      "[ 17  74  88 131 157 141 137 213 171 190 169 171 162 163 159]\n",
      "[ 17  74  88 129 158 141 136 213 169 191 173 169 164 160 161]\n",
      "[ 17  72  88 130 157 141 138 213 169 193 172 170 163 159 161]\n",
      "[ 17  69  87 132 159 139 138 215 166 190 171 171 169 161 159]\n",
      "[ 17  66  87 131 159 139 138 218 165 190 172 171 170 161 159]\n",
      "[ 17  64  89 129 157 139 136 220 168 191 172 173 172 157 159]\n",
      "[ 17  66  87 131 158 138 137 215 172 190 171 175 170 158 158]\n",
      "[ 17  66  85 131 154 137 138 219 173 193 169 174 169 160 158]\n",
      "[ 17  65  84 130 155 139 138 219 170 191 169 174 171 160 161]\n",
      "[ 17  66  85 130 154 138 136 221 169 193 167 173 171 162 161]\n",
      "[ 18  67  85 132 153 138 141 221 169 194 166 173 167 161 158]\n",
      "[ 17  67  86 132 154 140 139 225 170 194 168 172 163 159 157]\n",
      "[ 17  68  87 130 151 141 140 227 170 191 168 169 166 160 158]\n",
      "[ 17  69  89 125 150 141 138 227 172 193 169 168 166 160 159]\n",
      "[ 17  69  90 126 151 139 137 224 172 198 171 163 167 158 161]\n",
      "[ 17  70  87 127 149 138 139 223 174 196 172 165 167 159 160]\n",
      "[ 17  71  87 125 147 139 138 219 173 201 173 170 166 158 159]\n",
      "[ 17  70  87 127 149 138 136 222 171 200 172 168 167 161 158]\n",
      "[ 17  68  86 127 148 138 137 221 174 202 172 166 166 163 158]\n",
      "[ 17  68  87 124 148 137 137 223 172 197 171 169 167 167 159]\n",
      "[ 17  68  88 125 151 139 133 225 170 198 170 172 166 163 158]\n",
      "[ 15  67  90 126 150 141 134 220 172 198 169 171 167 161 162]\n",
      "[ 15  64  88 125 148 142 137 220 171 196 169 173 167 165 163]\n",
      "[ 15  63  87 124 149 142 138 220 171 199 169 176 168 164 158]\n",
      "[ 16  62  86 126 149 142 138 223 169 194 170 171 171 166 160]\n",
      "[ 15  63  88 126 150 142 137 223 170 197 169 169 170 168 156]\n",
      "[ 15  63  89 127 150 141 137 226 169 197 168 167 171 167 156]\n",
      "[ 14  62  90 125 152 141 134 227 168 197 168 171 172 165 157]\n",
      "[ 14  60  88 126 151 139 136 225 168 197 170 172 171 164 162]\n",
      "[ 14  58  88 127 150 141 136 229 164 200 169 170 172 165 160]\n",
      "[ 14  59  87 128 150 140 136 229 165 198 173 170 172 164 158]\n",
      "[ 14  57  89 130 151 143 130 229 169 198 173 168 172 164 156]\n",
      "[ 14  57  89 130 150 141 130 230 166 198 172 169 174 164 159]\n",
      "[ 15  55  89 130 150 140 130 232 165 200 171 172 174 162 158]\n",
      "[ 14  55  89 132 150 140 130 232 163 199 172 172 175 163 157]\n",
      "[ 14  56  89 132 150 143 130 232 163 200 169 169 175 163 158]\n",
      "[ 14  57  89 129 152 144 129 233 162 200 167 170 176 164 157]\n",
      "[ 14  56  87 128 151 144 131 234 163 200 170 164 176 166 159]\n",
      "[ 14  60  87 126 146 143 132 235 166 200 169 166 174 163 162]\n",
      "[ 14  58  88 123 146 144 131 234 169 202 169 165 174 166 160]\n",
      "[ 14  60  89 124 144 141 129 232 169 206 171 168 173 165 158]\n",
      "[ 14  60  94 123 143 141 130 232 168 208 171 167 172 162 158]\n",
      "[ 14  59  93 123 149 142 133 231 170 204 170 166 170 162 157]\n",
      "[ 14  61  91 123 149 142 134 232 171 204 165 171 166 162 158]\n",
      "[ 15  59  91 122 147 144 135 231 170 207 165 166 167 164 160]\n",
      "[ 14  59  92 120 150 143 133 231 167 207 168 167 166 164 162]\n",
      "[ 14  59  91 118 151 144 133 234 170 206 168 166 165 162 162]\n",
      "[ 14  59  93 121 151 142 131 234 171 203 169 168 166 160 161]\n",
      "[ 14  60  91 120 153 141 132 233 170 201 169 169 167 161 162]\n",
      "[ 14  63  89 121 153 142 133 233 168 206 166 169 167 159 160]\n",
      "[ 14  60  90 121 153 141 134 234 166 206 167 170 167 159 161]\n",
      "[ 14  57  90 125 154 143 131 235 166 205 169 168 166 160 160]\n",
      "[ 15  56  90 125 155 140 131 237 167 204 168 167 168 159 161]\n",
      "[ 15  55  89 127 159 139 129 234 169 206 169 165 167 160 160]\n",
      "[ 15  54  86 128 155 140 127 232 172 208 169 165 170 160 162]\n",
      "[ 16  56  89 128 154 140 130 231 172 204 167 164 172 160 160]\n",
      "[ 16  57  88 125 153 138 130 233 170 206 166 165 173 161 162]\n",
      "[ 16  59  86 124 155 139 129 232 172 208 169 163 171 159 161]\n",
      "[ 17  60  88 124 157 137 131 232 167 201 170 164 170 163 162]\n",
      "[ 16  62  87 124 156 138 132 235 164 199 170 164 169 165 162]\n",
      "[ 16  63  90 120 155 136 133 234 165 199 172 165 170 163 162]\n",
      "[ 16  62  90 121 156 136 135 234 167 201 172 161 170 161 161]\n",
      "[ 16  61  90 121 158 137 133 233 165 204 170 161 169 163 162]\n",
      "[ 17  63  91 122 159 136 132 232 163 203 170 159 169 164 163]\n",
      "[ 16  64  89 122 157 138 133 233 164 205 168 161 168 164 161]\n",
      "[ 16  62  90 122 160 138 132 231 167 206 167 160 169 162 161]\n",
      "[ 16  61  89 119 158 140 131 238 163 204 167 160 172 162 163]\n",
      "[ 16  61  87 121 156 140 132 236 164 204 168 159 172 168 159]\n",
      "[ 15  63  87 121 156 140 133 234 164 204 168 159 175 164 160]\n",
      "[ 15  62  87 123 155 141 132 234 165 204 167 159 176 164 159]\n",
      "[ 15  60  86 124 154 141 133 234 165 202 168 161 174 164 162]\n",
      "[ 14  60  84 129 152 138 135 237 162 202 170 162 172 166 160]\n",
      "[ 14  60  84 127 151 143 134 237 160 201 169 163 171 168 161]\n",
      "[ 14  58  83 127 152 145 136 237 162 200 169 161 169 170 160]\n",
      "[ 14  58  85 127 154 142 135 241 164 200 170 159 168 167 159]\n",
      "[ 13  58  86 127 154 143 135 241 166 199 169 157 170 167 158]\n",
      "[ 13  58  86 126 152 144 133 241 165 201 169 161 168 165 161]\n",
      "[ 13  58  86 128 153 142 134 243 163 199 169 159 169 167 160]\n",
      "[ 13  58  86 126 151 144 134 241 165 199 171 158 168 168 161]\n",
      "[ 13  59  87 122 148 145 135 241 166 199 172 156 170 168 162]\n",
      "[ 13  56  87 123 149 146 135 242 169 197 170 158 171 165 162]\n",
      "[ 13  57  85 123 149 145 132 242 171 199 170 159 171 166 161]\n",
      "[ 13  57  86 125 149 144 132 239 173 202 171 157 173 162 160]\n",
      "[ 13  60  87 126 146 143 135 240 172 200 166 159 173 162 161]\n",
      "[ 13  63  87 125 146 141 137 238 170 201 169 158 174 162 159]\n",
      "[ 13  64  87 128 147 146 134 238 167 201 170 156 173 161 158]\n",
      "[ 13  61  87 130 149 147 134 241 166 199 167 157 169 163 160]\n",
      "[ 13  62  87 134 149 147 132 237 167 200 168 155 170 164 158]\n",
      "[ 13  62  85 134 152 147 132 243 162 200 167 155 170 162 159]\n",
      "[ 15  65  88 135 150 145 129 241 164 199 166 154 172 162 158]\n",
      "[ 14  65  90 132 151 144 130 240 165 196 165 156 173 161 161]\n",
      "[ 13  62  90 134 148 147 134 238 166 197 163 159 175 160 157]\n",
      "[ 13  62  91 135 148 148 130 239 163 198 161 163 173 161 158]\n",
      "[ 13  62  91 136 149 147 132 238 165 200 162 159 169 163 157]\n",
      "[ 13  61  91 135 148 147 132 236 166 202 161 161 169 163 158]\n",
      "[ 13  61  91 136 146 145 132 240 166 200 163 160 170 162 158]\n",
      "[ 13  62  91 140 143 144 131 239 166 198 161 161 174 161 159]\n"
     ]
    }
   ],
   "source": [
    "doc_vectors = tweetHist.collect()\n",
    "for i in range(200):\n",
    "    print bincount(z)\n",
    "    pi = np.random.dirichlet(betas + bincount(z, minlength=K))\n",
    "    sum_z = np.zeros((K, V))\n",
    "    for i in range(no_tweets):\n",
    "        sum_z[z[i]]+=doc_vectors[i]\n",
    "    for i in range(K):\n",
    "        theta[i] = np.random.dirichlet(A[i]+sum_z[i])\n",
    "    for d in range(no_tweets):\n",
    "        for i in range(K):\n",
    "            pi1[i]= np.dot(np.log(theta[i]),doc_vectors[d])\n",
    "        pi1 = np.exp(pi1-np.max(pi1))\n",
    "        for i in range(K):\n",
    "            pi1[i] *= pi[i]\n",
    "        pi1 = pi1/np.sum(pi1)\n",
    "        #print pi1\n",
    "        z[d]=sampleFromDiscrete(pi1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1 -1 -1 ..., -1 -1 -1]\n"
     ]
    }
   ],
   "source": [
    "print z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 13  61  90 135 142 147 131 240 171 202 159 160 173 162 157]\n",
      "[ 7  9 12  8 13 11 10 14]\n"
     ]
    }
   ],
   "source": [
    "print bincount(z)\n",
    "freq_class = argsort(bincount(z))[::-1][: 8]\n",
    "print freq_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n",
      "[u'noth', u'time', u'think', u'politician', u'debat', u'america', u'run', u'clinton', u'fuck', u'american', u'realli', u'hillaryclinton', u'get', u'like', u'chicago', u'face', u'candid', u'cnn', u'stop', u'go', u'realdonaldtrump', u'kasich', u'cruz', u'right', u'sander', u'let', u'\\u2026', u'berniesand', u'berni', u'donald', u'ohio', u'demdeb', u'peopl', u'support', u'want', u'vote', u'hillari', u'amp', u'co', u'trump']\n",
      "[u'florida', u'twitter', u'take', u'line', u'citizen', u'hope', u'polit', u'street', u'one', u'lie', u'win', u'come', u'like', u'night', u'togeth', u'donald', u'youtub', u'best', u'2016', u'last', u'break', u'miami', u'univis', u'get', u'go', u'wall', u'clinton', u'via', u'hillaryclinton', u'democrat', u'berniesand', u'\\u2026', u'feelthebern', u'sander', u'hillari', u'trump', u'berni', u'debat', u'demdeb', u'co']\n",
      "[u'amp', u'oliv', u'question', u'anoth', u'debat', u'go', u'said', u'obama', u'student', u'demdeb', u'race', u'endors', u'ted', u'caitlyn', u'marco', u'republican', u'tackl', u'florida', u'realdonaldtrump', u'donald', u'power', u'black', u'democrat', u'say', u'video', u'kasich', u'sander', u'support', u'ralli', u'cruz', u'rubio', u'protest', u'polic', u'punch', u'clinton', u'hillari', u'berni', u'\\u2026', u'trump', u'co']\n",
      "[u'dagunnyandsuzii', u'attack', u'pleas', u'like', u'rnc', u'said', u'care', u'keep', u'everyth', u'think', u'sure', u'fact', u'nevercruz', u'democrat', u'marcorubio', u'2016elect', u'hillaryclinton', u'demdeb', u'feelthebern', u'realdonaldtrump', u'alwaystrump', u'trump', u'claim', u'behind', u'nevertrump', u'principl', u'debat', u'go', u'sander', u'candid', u'vote', u'berniesand', u'hillari', u'chang', u'parti', u'clinton', u'berni', u'\\u2026', u'gop', u'co']\n",
      "[u'ccot', u'cop', u'million', u'hillaryclinton', u'2', u'wall', u'allow', u'great', u'4', u'help', u'via', u'johnkstahlusa', u'surgeon', u'way', u'democrat', u'politician', u'\\u2026', u'make', u'amp', u'think', u'tcot', u'debat', u'berni', u'say', u'donald', u'never', u'c\\u2026', u'go', u'support', u'http\\u2026', u'vote', u'get', u'cruz', u'gop', u'sander', u'clinton', u'like', u'hillari', u'co', u'trump']\n",
      "[u'black', u'record', u'stori', u'ralli', u'divert', u'show', u'my\\u015ble\\u0107', u'live', u'tedcruz', u'pleas', u'question', u'c\\u2026', u'last', u'trumptrain', u'sander', u'clinton', u'ted', u'cruzcrew', u'think', u'support', u'man', u'throw', u'senat', u'trump2016', u'like', u'endors', u'donald', u'poll', u'realdonaldtrump', u'hillari', u'berniesand', u'feelthebern', u'via', u'presid', u'cruz', u'hate', u'berni', u'vote', u'trump', u'co']\n",
      "[u'hope', u'bout', u'want', u'sound', u'elect', u'say', u'berni', u'one', u'war', u'win', u'self', u'cruz', u'2', u'hillaryclinton', u'identifi', u'fund', u'tedcruz', u'berniesand', u'ur', u'shaunk', u'anarchist', u'star', u'know', u'hillari', u'face', u'trump2016', u'show', u'night', u'last', u'punch', u'repeatedli', u'substanc', u'realdonaldtrump', u'vote', u'u', u'support', u'campaign', u'co', u'\\u2026', u'trump']\n",
      "[u'republican', u'trade', u'para', u'es', u'nd', u'america', u'2016', u'news', u'gun', u'lie', u'like', u'primari', u'vote', u'year', u'jessthesav', u'el', u'que', u'lucha', u'obama', u'support', u'clinton', u'2', u'hillari', u'gop', u'fuck', u'u', u'say', u'amp', u'said', u'en', u'get', u'\\u2026', u'ted', u'berni', u'de', u'donald', u'cruz', u'la', u'co', u'trump']\n"
     ]
    }
   ],
   "source": [
    "print len(theta)\n",
    "indices = argsort(theta, axis = 1)\n",
    "best = indices[:,-40:]\n",
    "for k in freq_class:\n",
    "    wordlist = [vocab[best[k][j]] for j in range(len(best[0]))]          \n",
    "    print wordlist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Number of posts from each user partition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the Pickle file `../Data/users-partition.pickle`, you will get a dictionary which represents a partition over 452,743 Twitter users, `{user_id: partition_id}`. The users are partitioned into 7 groups. For example, if the dictionary is loaded into a variable named `partition`, the partition ID of the user `59458445` is `partition[\"59458445\"]`. These users are partitioned into 7 groups. The partition ID is an integer between 0-6.\n",
    "\n",
    "Note that the user partition we provide doesn't cover all users appear in the input data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1) Load the pickle file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# your code here\n",
    "import pickle\n",
    "file_Name = '../Data/users-partition.pickle'\n",
    "fileObject = open(file_Name,'r')  \n",
    "b = pickle.load(fileObject) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2) Count the number of posts from each user partition\n",
    "\n",
    "Count the number of posts from group 0, 1, ..., 6, plus the number of posts from users who are not in any partition. Assign users who are not in any partition to the group 7.\n",
    "\n",
    "Put the results of this step into a pair RDD `(group_id, count)` that is sorted by key."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# your code here\n",
    "default = 7\n",
    "counts_id = rdd2.map(lambda x: (b.get( x[0], default), 1))\\\n",
    "                .reduceByKey(lambda x,y:x+y).sortByKey()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(3) Print the post count using the `print_post_count` function we provided.\n",
    "\n",
    "It should print\n",
    "\n",
    "```\n",
    "Group 0 posted 81 tweets\n",
    "Group 1 posted 199 tweets\n",
    "Group 2 posted 45 tweets\n",
    "Group 3 posted 313 tweets\n",
    "Group 4 posted 86 tweets\n",
    "Group 5 posted 221 tweets\n",
    "Group 6 posted 400 tweets\n",
    "Group 7 posted 798 tweets\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_post_count(counts):\n",
    "    for group_id, count in counts:\n",
    "        print 'Group %d posted %d tweets' % (group_id, count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Part 3:  Tokens that are relatively popular in each user partition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In this step, we are going to find tokens that are relatively popular in each user partition.\n",
    "\n",
    "We define the number of mentions of a token $t$ in a specific user partition $k$ as the number of users from the user partition $k$ that ever mentioned the token $t$ in their tweets. Note that even if some users might mention a token $t$ multiple times or in multiple tweets, a user will contribute at most 1 to the counter of the token $t$.\n",
    "\n",
    "Please make sure that the number of mentions of a token is equal to the number of users who mentioned this token but NOT the number of tweets that mentioned this token.\n",
    "\n",
    "Let $N_t^k$ be the number of mentions of the token $t$ in the user partition $k$. Let $N_t^{all} = \\sum_{i=0}^7 N_t^{i}$ be the number of total mentions of the token $t$.\n",
    "\n",
    "We define the relative popularity of a token $t$ in a user partition $k$ as the log ratio between $N_t^k$ and $N_t^{all}$, i.e. \n",
    "\n",
    "\\begin{equation}\n",
    "p_t^k = \\log \\frac{C_t^k}{C_t^{all}}.\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "You can compute the relative popularity by calling the function `get_rel_popularity`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(0) Load the tweet tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load happyfuntokenizing.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "\"\"\"\n",
    "This code implements a basic, Twitter-aware tokenizer.\n",
    "\n",
    "A tokenizer is a function that splits a string of text into words. In\n",
    "Python terms, we map string and unicode objects into lists of unicode\n",
    "objects.\n",
    "\n",
    "There is not a single right way to do tokenizing. The best method\n",
    "depends on the application.  This tokenizer is designed to be flexible\n",
    "and this easy to adapt to new domains and tasks.  The basic logic is\n",
    "this:\n",
    "\n",
    "1. The tuple regex_strings defines a list of regular expression\n",
    "   strings.\n",
    "\n",
    "2. The regex_strings strings are put, in order, into a compiled\n",
    "   regular expression object called word_re.\n",
    "\n",
    "3. The tokenization is done by word_re.findall(s), where s is the\n",
    "   user-supplied string, inside the tokenize() method of the class\n",
    "   Tokenizer.\n",
    "\n",
    "4. When instantiating Tokenizer objects, there is a single option:\n",
    "   preserve_case.  By default, it is set to True. If it is set to\n",
    "   False, then the tokenizer will downcase everything except for\n",
    "   emoticons.\n",
    "\n",
    "The __main__ method illustrates by tokenizing a few examples.\n",
    "\n",
    "I've also included a Tokenizer method tokenize_random_tweet(). If the\n",
    "twitter library is installed (http://code.google.com/p/python-twitter/)\n",
    "and Twitter is cooperating, then it should tokenize a random\n",
    "English-language tweet.\n",
    "\n",
    "\n",
    "Julaiti Alafate:\n",
    "  I modified the regex strings to extract URLs in tweets.\n",
    "\"\"\"\n",
    "\n",
    "__author__ = \"Christopher Potts\"\n",
    "__copyright__ = \"Copyright 2011, Christopher Potts\"\n",
    "__credits__ = []\n",
    "__license__ = \"Creative Commons Attribution-NonCommercial-ShareAlike 3.0 Unported License: http://creativecommons.org/licenses/by-nc-sa/3.0/\"\n",
    "__version__ = \"1.0\"\n",
    "__maintainer__ = \"Christopher Potts\"\n",
    "__email__ = \"See the author's website\"\n",
    "\n",
    "######################################################################\n",
    "\n",
    "import re\n",
    "import htmlentitydefs\n",
    "\n",
    "######################################################################\n",
    "# The following strings are components in the regular expression\n",
    "# that is used for tokenizing. It's important that phone_number\n",
    "# appears first in the final regex (since it can contain whitespace).\n",
    "# It also could matter that tags comes after emoticons, due to the\n",
    "# possibility of having text like\n",
    "#\n",
    "#     <:| and some text >:)\n",
    "#\n",
    "# Most imporatantly, the final element should always be last, since it\n",
    "# does a last ditch whitespace-based tokenization of whatever is left.\n",
    "\n",
    "# This particular element is used in a couple ways, so we define it\n",
    "# with a name:\n",
    "emoticon_string = r\"\"\"\n",
    "    (?:\n",
    "      [<>]?\n",
    "      [:;=8]                     # eyes\n",
    "      [\\-o\\*\\']?                 # optional nose\n",
    "      [\\)\\]\\(\\[dDpP/\\:\\}\\{@\\|\\\\] # mouth      \n",
    "      |\n",
    "      [\\)\\]\\(\\[dDpP/\\:\\}\\{@\\|\\\\] # mouth\n",
    "      [\\-o\\*\\']?                 # optional nose\n",
    "      [:;=8]                     # eyes\n",
    "      [<>]?\n",
    "    )\"\"\"\n",
    "\n",
    "# The components of the tokenizer:\n",
    "regex_strings = (\n",
    "    # Phone numbers:\n",
    "    r\"\"\"\n",
    "    (?:\n",
    "      (?:            # (international)\n",
    "        \\+?[01]\n",
    "        [\\-\\s.]*\n",
    "      )?            \n",
    "      (?:            # (area code)\n",
    "        [\\(]?\n",
    "        \\d{3}\n",
    "        [\\-\\s.\\)]*\n",
    "      )?    \n",
    "      \\d{3}          # exchange\n",
    "      [\\-\\s.]*   \n",
    "      \\d{4}          # base\n",
    "    )\"\"\"\n",
    "    ,\n",
    "    # URLs:\n",
    "    r\"\"\"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\"\"\"\n",
    "    ,\n",
    "    # Emoticons:\n",
    "    emoticon_string\n",
    "    ,    \n",
    "    # HTML tags:\n",
    "     r\"\"\"<[^>]+>\"\"\"\n",
    "    ,\n",
    "    # Twitter username:\n",
    "    r\"\"\"(?:@[\\w_]+)\"\"\"\n",
    "    ,\n",
    "    # Twitter hashtags:\n",
    "    r\"\"\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\"\"\"\n",
    "    ,\n",
    "    # Remaining word types:\n",
    "    r\"\"\"\n",
    "    (?:[a-z][a-z'\\-_]+[a-z])       # Words with apostrophes or dashes.\n",
    "    |\n",
    "    (?:[+\\-]?\\d+[,/.:-]\\d+[+\\-]?)  # Numbers, including fractions, decimals.\n",
    "    |\n",
    "    (?:[\\w_]+)                     # Words without apostrophes or dashes.\n",
    "    |\n",
    "    (?:\\.(?:\\s*\\.){1,})            # Ellipsis dots. \n",
    "    |\n",
    "    (?:\\S)                         # Everything else that isn't whitespace.\n",
    "    \"\"\"\n",
    "    )\n",
    "\n",
    "######################################################################\n",
    "# This is the core tokenizing regex:\n",
    "    \n",
    "word_re = re.compile(r\"\"\"(%s)\"\"\" % \"|\".join(regex_strings), re.VERBOSE | re.I | re.UNICODE)\n",
    "\n",
    "# The emoticon string gets its own regex so that we can preserve case for them as needed:\n",
    "emoticon_re = re.compile(regex_strings[1], re.VERBOSE | re.I | re.UNICODE)\n",
    "\n",
    "# These are for regularizing HTML entities to Unicode:\n",
    "html_entity_digit_re = re.compile(r\"&#\\d+;\")\n",
    "html_entity_alpha_re = re.compile(r\"&\\w+;\")\n",
    "amp = \"&amp;\"\n",
    "\n",
    "######################################################################\n",
    "\n",
    "class Tokenizer:\n",
    "    def __init__(self, preserve_case=False):\n",
    "        self.preserve_case = preserve_case\n",
    "\n",
    "    def tokenize(self, s):\n",
    "        \"\"\"\n",
    "        Argument: s -- any string or unicode object\n",
    "        Value: a tokenize list of strings; conatenating this list returns the original string if preserve_case=False\n",
    "        \"\"\"        \n",
    "        # Try to ensure unicode:\n",
    "        try:\n",
    "            s = unicode(s)\n",
    "        except UnicodeDecodeError:\n",
    "            s = str(s).encode('string_escape')\n",
    "            s = unicode(s)\n",
    "        # Fix HTML character entitites:\n",
    "        s = self.__html2unicode(s)\n",
    "        # Tokenize:\n",
    "        words = word_re.findall(s)\n",
    "        # Possible alter the case, but avoid changing emoticons like :D into :d:\n",
    "        if not self.preserve_case:            \n",
    "            words = map((lambda x : x if emoticon_re.search(x) else x.lower()), words)\n",
    "        return words\n",
    "\n",
    "    def tokenize_random_tweet(self):\n",
    "        \"\"\"\n",
    "        If the twitter library is installed and a twitter connection\n",
    "        can be established, then tokenize a random tweet.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            import twitter\n",
    "        except ImportError:\n",
    "            print \"Apologies. The random tweet functionality requires the Python twitter library: http://code.google.com/p/python-twitter/\"\n",
    "        from random import shuffle\n",
    "        api = twitter.Api()\n",
    "        tweets = api.GetPublicTimeline()\n",
    "        if tweets:\n",
    "            for tweet in tweets:\n",
    "                if tweet.user.lang == 'en':            \n",
    "                    return self.tokenize(tweet.text)\n",
    "        else:\n",
    "            raise Exception(\"Apologies. I couldn't get Twitter to give me a public English-language tweet. Perhaps try again\")\n",
    "\n",
    "    def __html2unicode(self, s):\n",
    "        \"\"\"\n",
    "        Internal metod that seeks to replace all the HTML entities in\n",
    "        s with their corresponding unicode characters.\n",
    "        \"\"\"\n",
    "        # First the digits:\n",
    "        ents = set(html_entity_digit_re.findall(s))\n",
    "        if len(ents) > 0:\n",
    "            for ent in ents:\n",
    "                entnum = ent[2:-1]\n",
    "                try:\n",
    "                    entnum = int(entnum)\n",
    "                    s = s.replace(ent, unichr(entnum))\t\n",
    "                except:\n",
    "                    pass\n",
    "        # Now the alpha versions:\n",
    "        ents = set(html_entity_alpha_re.findall(s))\n",
    "        ents = filter((lambda x : x != amp), ents)\n",
    "        for ent in ents:\n",
    "            entname = ent[1:-1]\n",
    "            try:            \n",
    "                s = s.replace(ent, unichr(htmlentitydefs.name2codepoint[entname]))\n",
    "            except:\n",
    "                pass                    \n",
    "            s = s.replace(amp, \" and \")\n",
    "        return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "from math import log\n",
    "\n",
    "tok = Tokenizer(preserve_case=False)\n",
    "\n",
    "def get_rel_popularity(c_k, c_all):\n",
    "    return log(1.0 * c_k / c_all) / log(2)\n",
    "\n",
    "\n",
    "def print_tokens(tokens, gid = None):\n",
    "    group_name = \"overall\"\n",
    "    if gid is not None:\n",
    "        group_name = \"group %d\" % gid\n",
    "    print '=' * 5 + ' ' + group_name + ' ' + '=' * 5\n",
    "    for n, t in tokens:\n",
    "        print \"%s\\t%.4f\" % (t.encode('utf-8'), -n)\n",
    "    print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1) Tokenize the tweets using the tokenizer we provided above named `tok`. Count the number of mentions for each tokens regardless of specific user group.\n",
    "\n",
    "Call `print_count` function to show how many different tokens we have.\n",
    "\n",
    "It should print\n",
    "```\n",
    "Number of elements: 8949\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# your code here\n",
    "rdd_token2 = rdd2.mapValues(lambda x: tok.tokenize(x.encode('utf-8')))\\\n",
    "                .reduceByKey(lambda a, b: a+b).mapValues(lambda x: list(set(x)))\\\n",
    "                .map(lambda (c,v): (b.get( c, default),v))\n",
    "rdd_freq= rdd_token2.flatMap(lambda (c, v) : v).map(lambda x: (x,1)).reduceByKey(lambda a,b:a+b)\n",
    "print_count(rdd_freq)\n",
    "#rdd_token2.take(2)\n",
    "rdd_token2.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2) Tokens that are mentioned by too few users are usually not very interesting. So we want to only keep tokens that are mentioned by at least 100 users. Please filter out tokens that don't meet this requirement.\n",
    "\n",
    "Call `print_count` function to show how many different tokens we have after the filtering.\n",
    "\n",
    "Call `print_tokens` function to show top 20 most frequent tokens.\n",
    "\n",
    "It should print\n",
    "```\n",
    "Number of elements: 44\n",
    "===== overall =====\n",
    ":\t1388.0000\n",
    "rt\t1237.0000\n",
    ".\t826.0000\n",
    "…\t673.0000\n",
    "the\t623.0000\n",
    "trump\t582.0000\n",
    "to\t499.0000\n",
    ",\t489.0000\n",
    "a\t404.0000\n",
    "is\t376.0000\n",
    "in\t297.0000\n",
    "of\t292.0000\n",
    "and\t288.0000\n",
    "for\t281.0000\n",
    "!\t269.0000\n",
    "?\t210.0000\n",
    "on\t195.0000\n",
    "i\t192.0000\n",
    "you\t191.0000\n",
    "this\t190.0000\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# your code here\n",
    "rdd_filt = rdd_freq.filter(lambda (c,v): v >=100)\n",
    "freq_sort = rdd_filt.sortBy(lambda x: (-x[1], x[0])).map(lambda (c,v): (-v, c))\n",
    "print_count(rdd_filt)\n",
    "print_tokens(freq_sort.take(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(3) For all tokens that are mentioned by at least 100 users, compute their relative popularity in each user group. Then print the top 10 tokens with highest relative popularity in each user group. In case two tokens have same relative popularity, break the tie by printing the alphabetically smaller one.\n",
    "\n",
    "**Hint:** Let the relative popularity of a token $t$ be $p$. The order of the items will be satisfied by sorting them using (-p, t) as the key.\n",
    "\n",
    "It should print\n",
    "```\n",
    "===== group 0 =====\n",
    "...\t-3.5648\n",
    "at\t-3.5983\n",
    "hillary\t-4.0875\n",
    "i\t-4.1255\n",
    "bernie\t-4.1699\n",
    "not\t-4.2479\n",
    "https\t-4.2695\n",
    "he\t-4.2801\n",
    "in\t-4.3074\n",
    "are\t-4.3646\n",
    "\n",
    "===== group 1 =====\n",
    "#demdebate\t-2.4391\n",
    "-\t-2.6202\n",
    "&\t-2.7472\n",
    "amp\t-2.7472\n",
    "clinton\t-2.7570\n",
    ";\t-2.7980\n",
    "sanders\t-2.8838\n",
    "?\t-2.9069\n",
    "in\t-2.9664\n",
    "if\t-3.0138\n",
    "\n",
    "===== group 2 =====\n",
    "are\t-4.6865\n",
    "and\t-4.7105\n",
    "bernie\t-4.7549\n",
    "at\t-4.7682\n",
    "sanders\t-4.9542\n",
    "that\t-5.0224\n",
    "in\t-5.0444\n",
    "donald\t-5.0618\n",
    "a\t-5.0732\n",
    "#demdebate\t-5.1396\n",
    "\n",
    "===== group 3 =====\n",
    "#demdebate\t-1.3847\n",
    "bernie\t-1.8480\n",
    "sanders\t-2.1887\n",
    "of\t-2.2356\n",
    "that\t-2.3785\n",
    "the\t-2.4376\n",
    "…\t-2.4403\n",
    "clinton\t-2.4467\n",
    "hillary\t-2.4594\n",
    "be\t-2.5465\n",
    "\n",
    "===== group 4 =====\n",
    "hillary\t-3.7395\n",
    "sanders\t-3.9542\n",
    "of\t-4.0199\n",
    "clinton\t-4.0790\n",
    "at\t-4.1832\n",
    "in\t-4.2143\n",
    "a\t-4.2659\n",
    "on\t-4.2854\n",
    ".\t-4.3681\n",
    "the\t-4.4251\n",
    "\n",
    "===== group 5 =====\n",
    "cruz\t-2.3861\n",
    "he\t-2.6280\n",
    "are\t-2.7796\n",
    "will\t-2.7829\n",
    "the\t-2.8568\n",
    "is\t-2.8822\n",
    "for\t-2.9250\n",
    "that\t-2.9349\n",
    "of\t-2.9804\n",
    "this\t-2.9849\n",
    "\n",
    "===== group 6 =====\n",
    "@realdonaldtrump\t-1.1520\n",
    "cruz\t-1.4532\n",
    "https\t-1.5222\n",
    "!\t-1.5479\n",
    "not\t-1.8904\n",
    "…\t-1.9269\n",
    "will\t-2.0124\n",
    "it\t-2.0345\n",
    "this\t-2.1104\n",
    "to\t-2.1685\n",
    "\n",
    "===== group 7 =====\n",
    "donald\t-0.6422\n",
    "...\t-0.7922\n",
    "sanders\t-1.0282\n",
    "trump\t-1.1296\n",
    "bernie\t-1.2106\n",
    "-\t-1.2253\n",
    "you\t-1.2376\n",
    "clinton\t-1.2511\n",
    "if\t-1.2880\n",
    "i\t-1.2996\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# your code here\n",
    "rdd_group= rdd_token2.flatMapValues(lambda v : v).map(lambda (c,v): (v,c))\\\n",
    "            .map(lambda x: (x,1)).reduceByKey(lambda a,b:a+b)\n",
    "#rdd_group.take(5)\n",
    "def getKey(item):\n",
    "    return item[1]\n",
    "#, key=getKey, reverse=True\n",
    "\n",
    "rdd_joined = rdd_filt.join(rdd_group.map(lambda ((t, w), u): (t, (w,u))))\\\n",
    "              .map(lambda (t, (v, (w, u))): ((t, w), (u, v)))\n",
    "rdd_grouped = rdd_joined.mapValues(lambda x: log(1.0 * x[0] / x[1]) / log(2))\\\n",
    "                   .map(lambda ((t, w), u): (w, (-u,t))).groupByKey()\\\n",
    "                    .mapValues(lambda x: sorted(x))\n",
    "        #.mapValues(lambda (c,v): (v, -c))\n",
    "#rdd_grouped.take(8)        \n",
    "for n in range(8):\n",
    "    print_tokens(rdd_grouped.collectAsMap()[n][0:10],n)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "(4) (optional, not for grading) The users partition is generated by a machine learning algorithm that tries to group the users by their political preferences. Three of the user groups are showing supports to Bernie Sanders, Ted Cruz, and Donald Trump. \n",
    "\n",
    "If your program looks okay on the local test data, you can try it on the larger input by submitting your program to the homework server. Observe the output of your program to larger input files, can you guess the partition IDs of the three groups mentioned above based on your output?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Change the values of the following three items to your guesses\n",
    "\n",
    "users_support = [\n",
    "    (-1, \"Bernie Sanders\"),\n",
    "    (-1, \"Ted Cruz\"),\n",
    "    (-1, \"Donald Trump\")\n",
    "]\n",
    "\n",
    "for gid, candidate in users_support:\n",
    "    print \"Users from group %d are most likely to support %s.\" % (gid, candidate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
